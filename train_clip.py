# train_clip.py
from transformers import CLIPProcessor, CLIPModel
from utils.data_utils import get_dataloader
from utils.train_utils import train_clip
from config import CONFIG

print("π“¦ λ¨λΈ λ΅λ”© μ¤‘...")
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

print("π”§ λ°μ΄ν„° λ΅λ”© μ¤‘...")
dataloader, class_names = get_dataloader(CONFIG["image_root"], CONFIG["batch_size"])

print(f"ν΄λμ¤ μ: {len(class_names)}")

print("π€ νμΈνλ‹ μ‹μ‘!")
train_clip(model, processor, dataloader, class_names, CONFIG)
